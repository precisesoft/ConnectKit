name: Nightly Extended Tests

on:
  schedule:
    # Run every night at 2 AM UTC
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      test_suite:
        description: "Test suite to run"
        required: true
        default: "all"
        type: choice
        options:
          - all
          - performance
          - security
          - chaos
          - data-integrity
          - backup-restore
      duration:
        description: "Test duration (for performance tests)"
        default: "30m"
        type: string

env:
  NODE_VERSION: "18"
  EXTENDED_TEST_MODE: "true"

jobs:
  # Extended performance testing with longer duration
  extended-performance:
    name: Extended Performance Testing
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'performance' || github.event_name == 'schedule' }}
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v4

      - name: Setup k6
        run: |
          wget https://github.com/grafana/k6/releases/download/v0.47.0/k6-v0.47.0-linux-amd64.tar.gz
          tar -xzf k6-v0.47.0-linux-amd64.tar.gz
          sudo cp k6-v0.47.0-linux-amd64/k6 /usr/local/bin/

      - name: Start services with production-like config
        run: |
          cp .env.example .env
          # Add production-like settings
          echo "NODE_ENV=production" >> .env
          echo "LOG_LEVEL=warn" >> .env
          docker-compose up -d
          sleep 60

      - name: Wait for services to be ready
        run: |
          timeout 120 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 5; done'

      - name: Create extended performance test data
        run: |
          # Seed database with substantial test data for realistic testing
          cat > seed-large-dataset.js << 'EOF'
          const http = require('http');
          const https = require('https');

          async function seedLargeDataset() {
            const baseUrl = 'http://localhost:3001';
            
            // Register admin user
            const adminData = {
              email: 'admin@nightlytest.com',
              username: 'nightlyadmin',
              password: 'NightlyTest123!',
              firstName: 'Nightly',
              lastName: 'Admin'
            };
            
            console.log('Creating admin user...');
            
            const registerResponse = await fetch(`${baseUrl}/api/auth/register`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify(adminData)
            });
            
            if (registerResponse.ok) {
              const { data } = await registerResponse.json();
              const token = data.token;
              
              console.log('Creating test contacts...');
              
              // Create 1000+ contacts for realistic load testing
              const promises = [];
              for (let i = 0; i < 1000; i++) {
                const contact = {
                  firstName: `TestContact${i}`,
                  lastName: `Nightly${Math.floor(i / 100)}`,
                  email: `nightly-${i}@test.example.com`,
                  company: `TestCorp${Math.floor(i / 50)}`,
                  jobTitle: `Position ${i % 20}`,
                  phone: `+1${String(2000000000 + i).substring(0, 10)}`,
                  notes: `Generated for nightly testing - batch ${Math.floor(i / 100)}`,
                  tags: ['nightly-test', `batch-${Math.floor(i / 100)}`, `wave-${Math.floor(i / 200)}`]
                };
                
                promises.push(
                  fetch(`${baseUrl}/api/contacts`, {
                    method: 'POST',
                    headers: {
                      'Content-Type': 'application/json',
                      'Authorization': `Bearer ${token}`
                    },
                    body: JSON.stringify(contact)
                  })
                );
                
                // Batch requests to avoid overwhelming the server
                if (promises.length >= 50) {
                  await Promise.all(promises);
                  promises.length = 0;
                  console.log(`Created ${i + 1} contacts...`);
                }
              }
              
              // Handle remaining promises
              if (promises.length > 0) {
                await Promise.all(promises);
              }
              
              console.log('Large dataset seeding completed!');
            } else {
              console.error('Failed to create admin user');
            }
          }

          seedLargeDataset().catch(console.error);
          EOF

          node seed-large-dataset.js

      - name: Run extended load test
        run: |
          DURATION="${{ github.event.inputs.duration || '30m' }}"
          echo "Running extended performance test for duration: $DURATION"

          # Create extended load test configuration
          cat > extended-load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend, Counter } from 'k6/metrics';

          // Extended test metrics
          const errorRate = new Rate('errors');
          const longRunningQueries = new Trend('long_running_queries');
          const memoryLeakIndicator = new Counter('potential_memory_leaks');

          export const options = {
            stages: [
              { duration: '5m', target: 50 },   // Warm up
              { duration: '10m', target: 100 }, // Normal load
              { duration: '10m', target: 200 }, // High load
              { duration: '5m', target: 500 },  // Spike test
              { duration: '15m', target: 200 }, // Sustained load
              { duration: '10m', target: 100 }, // Scale down
              { duration: '5m', target: 0 },    // Cool down
            ],
            thresholds: {
              http_req_duration: ['p(95)<1000'], // 95% under 1s for extended test
              errors: ['rate<0.1'], // 10% error rate acceptable for nightly
              long_running_queries: ['p(90)<2000'], // 90% of complex queries under 2s
            },
          };

          const BASE_URL = __ENV.BASE_URL || 'http://localhost:3001';

          export default function() {
            const operations = [
              searchLargeDataset,
              complexContactQueries,
              bulkContactOperations,
              paginationStressTest,
              concurrentUserSimulation
            ];
            
            const operation = operations[Math.floor(Math.random() * operations.length)];
            operation();
            
            sleep(Math.random() * 2 + 0.5); // Variable sleep 0.5-2.5s
          }

          function searchLargeDataset() {
            // Login first
            const loginResponse = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
              email: 'admin@nightlytest.com',
              password: 'NightlyTest123!'
            }), {
              headers: { 'Content-Type': 'application/json' },
            });
            
            if (loginResponse.status !== 200) return;
            
            const token = JSON.parse(loginResponse.body).data.token;
            const headers = { 'Authorization': `Bearer ${token}` };
            
            // Complex search queries on large dataset
            const searchTerms = ['TestContact', 'Nightly', 'TestCorp', 'Position', 'batch'];
            const searchTerm = searchTerms[Math.floor(Math.random() * searchTerms.length)];
            
            const startTime = Date.now();
            const response = http.get(`${BASE_URL}/api/contacts?search=${searchTerm}&limit=100`, { headers });
            const duration = Date.now() - startTime;
            
            longRunningQueries.add(duration);
            
            check(response, {
              'large dataset search successful': (r) => r.status === 200,
              'search returns results': (r) => {
                try {
                  const data = JSON.parse(r.body);
                  return data.data && data.data.contacts && data.data.contacts.length > 0;
                } catch (e) {
                  return false;
                }
              },
            });
            
            errorRate.add(response.status >= 400);
          }

          function complexContactQueries() {
            const loginResponse = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
              email: 'admin@nightlytest.com',
              password: 'NightlyTest123!'
            }), {
              headers: { 'Content-Type': 'application/json' },
            });
            
            if (loginResponse.status !== 200) return;
            
            const token = JSON.parse(loginResponse.body).data.token;
            const headers = { 'Authorization': `Bearer ${token}` };
            
            // Multiple complex queries
            const queries = [
              'company=TestCorp1&status=active&limit=50',
              'tags=nightly-test&limit=75',
              'search=batch&company=TestCorp&limit=25',
              'sort=firstName&order=desc&limit=100'
            ];
            
            queries.forEach(query => {
              const response = http.get(`${BASE_URL}/api/contacts?${query}`, { headers });
              check(response, {
                'complex query successful': (r) => r.status === 200,
              });
              errorRate.add(response.status >= 400);
            });
          }

          function bulkContactOperations() {
            const loginResponse = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
              email: 'admin@nightlytest.com',
              password: 'NightlyTest123!'
            }), {
              headers: { 'Content-Type': 'application/json' },
            });
            
            if (loginResponse.status !== 200) return;
            
            const token = JSON.parse(loginResponse.body).data.token;
            const headers = { 
              'Authorization': `Bearer ${token}`,
              'Content-Type': 'application/json'
            };
            
            // Create multiple contacts rapidly
            for (let i = 0; i < 5; i++) {
              const contact = {
                firstName: `NightlyBulk${Date.now()}${i}`,
                lastName: `Extended${i}`,
                email: `bulk-${Date.now()}-${i}@nightly.test`,
                company: `BulkTestCorp${i}`,
                notes: `Created during extended nightly testing - iteration ${i}`
              };
              
              const response = http.post(`${BASE_URL}/api/contacts`, JSON.stringify(contact), { headers });
              check(response, {
                'bulk create successful': (r) => r.status === 201,
              });
              errorRate.add(response.status >= 400);
            }
          }

          function paginationStressTest() {
            const loginResponse = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({
              email: 'admin@nightlytest.com',
              password: 'NightlyTest123!'
            }), {
              headers: { 'Content-Type': 'application/json' },
            });
            
            if (loginResponse.status !== 200) return;
            
            const token = JSON.parse(loginResponse.body).data.token;
            const headers = { 'Authorization': `Bearer ${token}` };
            
            // Test various pagination scenarios
            const pageTests = [
              { page: 1, limit: 10 },
              { page: 5, limit: 20 },
              { page: 10, limit: 50 },
              { page: 20, limit: 25 },
              { page: 1, limit: 100 }
            ];
            
            pageTests.forEach(({ page, limit }) => {
              const response = http.get(`${BASE_URL}/api/contacts?page=${page}&limit=${limit}`, { headers });
              check(response, {
                'pagination successful': (r) => r.status === 200,
                'pagination has metadata': (r) => {
                  try {
                    const data = JSON.parse(r.body);
                    return data.data && data.data.pagination;
                  } catch (e) {
                    return false;
                  }
                },
              });
              errorRate.add(response.status >= 400);
            });
          }

          function concurrentUserSimulation() {
            // Simulate different user creating account and immediately using system
            const timestamp = Date.now();
            const userId = Math.random().toString(36).substring(7);
            
            const newUser = {
              email: `concurrent-${timestamp}-${userId}@nightly.test`,
              username: `concurrent${timestamp}${userId}`,
              password: 'ConcurrentTest123!',
              firstName: `Concurrent${userId}`,
              lastName: 'User'
            };
            
            // Register
            const registerResponse = http.post(`${BASE_URL}/api/auth/register`, JSON.stringify(newUser), {
              headers: { 'Content-Type': 'application/json' },
            });
            
            if (registerResponse.status === 201) {
              const token = JSON.parse(registerResponse.body).data.token;
              const headers = { 
                'Authorization': `Bearer ${token}`,
                'Content-Type': 'application/json'
              };
              
              // Immediately start using system
              http.get(`${BASE_URL}/api/contacts`, { headers });
              
              const contact = {
                firstName: 'Quick',
                lastName: 'Contact',
                email: `quick-${timestamp}@example.com`
              };
              
              http.post(`${BASE_URL}/api/contacts`, JSON.stringify(contact), { headers });
            }
            
            errorRate.add(registerResponse.status >= 400);
          }
          EOF

          k6 run extended-load-test.js --env BASE_URL=http://localhost:3001

      - name: Run memory leak detection
        run: |
          echo "## Extended Performance Test Results" >> $GITHUB_STEP_SUMMARY

          # Monitor memory usage during the test
          docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}" > performance-stats.txt

          echo "### System Resource Usage" >> $GITHUB_STEP_SUMMARY
          cat performance-stats.txt >> $GITHUB_STEP_SUMMARY

          # Check for potential memory leaks
          BACKEND_MEMORY=$(docker stats connectkit-backend --no-stream --format "{{.MemUsage}}")
          echo "**Backend Memory Usage:** $BACKEND_MEMORY" >> $GITHUB_STEP_SUMMARY

          # Performance test completed
          echo "✅ Extended performance testing completed" >> $GITHUB_STEP_SUMMARY

      - name: Upload extended performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: extended-performance-results
          path: |
            performance-stats.txt
            *.html

      - name: Stop services
        if: always()
        run: docker-compose down -v

  # Chaos engineering tests
  chaos-testing:
    name: Chaos Engineering Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'chaos' || github.event_name == 'schedule' }}
    timeout-minutes: 60

    steps:
      - uses: actions/checkout@v4

      - name: Start services
        run: |
          cp .env.example .env
          docker-compose up -d
          sleep 30

      - name: Wait for services
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 2; done'

      - name: Install chaos testing tools
        run: |
          # Install toxiproxy for network chaos
          wget https://github.com/Shopify/toxiproxy/releases/download/v2.5.0/toxiproxy-server-linux-amd64
          chmod +x toxiproxy-server-linux-amd64
          ./toxiproxy-server-linux-amd64 &

          wget https://github.com/Shopify/toxiproxy/releases/download/v2.5.0/toxiproxy-cli-linux-amd64
          chmod +x toxiproxy-cli-linux-amd64
          alias toxiproxy-cli='./toxiproxy-cli-linux-amd64'

      - name: Test database connection failures
        run: |
          echo "## Chaos Engineering Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Test 1: Database connection interruption
          echo "### Database Connection Chaos Test" >> $GITHUB_STEP_SUMMARY

          # Stop database container
          docker stop connectkit-db

          # Test how backend handles database unavailability
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:3001/api/health || echo "000")

          if [ "$RESPONSE" = "503" ] || [ "$RESPONSE" = "500" ]; then
            echo "✅ Backend gracefully handles database disconnection (HTTP $RESPONSE)" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Backend response to database failure: HTTP $RESPONSE" >> $GITHUB_STEP_SUMMARY
          fi

          # Restart database
          docker start connectkit-db
          sleep 30

          # Test recovery
          timeout 60 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 2; done'
          echo "✅ Service recovered after database restart" >> $GITHUB_STEP_SUMMARY

      - name: Test Redis connection failures
        run: |
          echo "### Redis Connection Chaos Test" >> $GITHUB_STEP_SUMMARY

          # Stop Redis container
          docker stop connectkit-redis

          # Test how backend handles Redis unavailability
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:3001/api/health || echo "000")

          if [ "$RESPONSE" = "200" ]; then
            echo "✅ Backend continues functioning without Redis (HTTP $RESPONSE)" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Backend affected by Redis failure: HTTP $RESPONSE" >> $GITHUB_STEP_SUMMARY
          fi

          # Restart Redis
          docker start connectkit-redis
          sleep 15

      - name: Test high CPU load
        run: |
          echo "### CPU Stress Test" >> $GITHUB_STEP_SUMMARY

          # Install stress testing tools
          sudo apt-get update
          sudo apt-get install -y stress-ng

          # Apply CPU stress for 60 seconds
          stress-ng --cpu 2 --timeout 60s &
          STRESS_PID=$!

          sleep 10

          # Test API responsiveness under CPU stress
          RESPONSE_TIME=$(curl -o /dev/null -s -w "%{time_total}" http://localhost:3001/api/health)

          if (( $(echo "$RESPONSE_TIME < 2.0" | bc -l) )); then
            echo "✅ API responsive under CPU stress (${RESPONSE_TIME}s response time)" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ API slow under CPU stress (${RESPONSE_TIME}s response time)" >> $GITHUB_STEP_SUMMARY
          fi

          # Wait for stress test to complete
          wait $STRESS_PID

      - name: Test memory pressure
        run: |
          echo "### Memory Pressure Test" >> $GITHUB_STEP_SUMMARY

          # Apply memory pressure
          stress-ng --vm 1 --vm-bytes 1G --timeout 30s &
          STRESS_PID=$!

          sleep 10

          # Test API under memory pressure
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:3001/api/health)

          if [ "$RESPONSE" = "200" ]; then
            echo "✅ API stable under memory pressure" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ API affected by memory pressure (HTTP $RESPONSE)" >> $GITHUB_STEP_SUMMARY
          fi

          wait $STRESS_PID

      - name: Test network latency simulation
        run: |
          echo "### Network Latency Simulation" >> $GITHUB_STEP_SUMMARY

          # Simulate network delays using tc (traffic control)
          sudo tc qdisc add dev lo root handle 1: prio
          sudo tc qdisc add dev lo parent 1:1 handle 10: netem delay 100ms

          # Test API with network delay
          START_TIME=$(date +%s.%N)
          curl -s http://localhost:3001/api/health > /dev/null
          END_TIME=$(date +%s.%N)
          RESPONSE_TIME=$(echo "$END_TIME - $START_TIME" | bc)

          if (( $(echo "$RESPONSE_TIME > 0.1" | bc -l) )); then
            echo "✅ Network latency simulation working (${RESPONSE_TIME}s response)" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Network latency simulation may not be working" >> $GITHUB_STEP_SUMMARY
          fi

          # Remove network delay
          sudo tc qdisc del dev lo root

      - name: Test cascading failure recovery
        run: |
          echo "### Cascading Failure Recovery Test" >> $GITHUB_STEP_SUMMARY

          # Stop all dependencies simultaneously
          docker stop connectkit-db connectkit-redis

          # Wait for potential cascading effects
          sleep 20

          # Check if backend is still running (should be degraded but not crashed)
          if docker ps --format "table {{.Names}}" | grep -q connectkit-backend; then
            echo "✅ Backend container survived dependency failures" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Backend container crashed due to dependency failures" >> $GITHUB_STEP_SUMMARY
          fi

          # Restart dependencies in wrong order (Redis first, then DB)
          docker start connectkit-redis
          sleep 10
          docker start connectkit-db
          sleep 30

          # Test full recovery
          timeout 60 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 2; done'
          echo "✅ System recovered from cascading failure" >> $GITHUB_STEP_SUMMARY

      - name: Stop services
        if: always()
        run: docker-compose down -v

  # Data integrity testing
  data-integrity:
    name: Data Integrity Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'data-integrity' || github.event_name == 'schedule' }}
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: integrity_test
          POSTGRES_PASSWORD: integrity_test
          POSTGRES_DB: connectkit_integrity
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install backend dependencies
        working-directory: ./backend
        run: npm ci

      - name: Run database migrations
        working-directory: ./backend
        env:
          NODE_ENV: test
          DB_HOST: localhost
          DB_PORT: 5432
          DB_USER: integrity_test
          DB_PASSWORD: integrity_test
          DB_NAME: connectkit_integrity
        run: npm run db:migrate

      - name: Create data integrity test suite
        working-directory: ./backend
        run: |
          cat > data-integrity-test.js << 'EOF'
          const { Pool } = require('pg');
          const crypto = require('crypto');

          const pool = new Pool({
            host: 'localhost',
            port: 5432,
            user: 'integrity_test',
            password: 'integrity_test',
            database: 'connectkit_integrity'
          });

          async function runDataIntegrityTests() {
            console.log('🔍 Starting data integrity tests...');
            
            try {
              // Test 1: Foreign key constraints
              console.log('\n1. Testing foreign key constraints...');
              await testForeignKeyConstraints();
              
              // Test 2: Data validation constraints
              console.log('\n2. Testing data validation constraints...');
              await testDataValidationConstraints();
              
              // Test 3: Concurrent transaction handling
              console.log('\n3. Testing concurrent transactions...');
              await testConcurrentTransactions();
              
              // Test 4: Data consistency under load
              console.log('\n4. Testing data consistency under load...');
              await testDataConsistencyUnderLoad();
              
              // Test 5: Audit trail integrity
              console.log('\n5. Testing audit trail integrity...');
              await testAuditTrailIntegrity();
              
              console.log('\n✅ All data integrity tests completed');
              
            } catch (error) {
              console.error('❌ Data integrity test failed:', error);
              process.exit(1);
            } finally {
              await pool.end();
            }
          }

          async function testForeignKeyConstraints() {
            const client = await pool.connect();
            
            try {
              // Try to insert contact with invalid user_id
              await client.query('BEGIN');
              
              try {
                await client.query(`
                  INSERT INTO contacts (user_id, first_name, last_name, email)
                  VALUES ('00000000-0000-0000-0000-000000000000', 'Test', 'User', 'test@example.com')
                `);
                throw new Error('Foreign key constraint should have been violated');
              } catch (error) {
                if (error.message.includes('foreign key constraint')) {
                  console.log('✅ Foreign key constraint working correctly');
                } else {
                  throw error;
                }
              }
              
              await client.query('ROLLBACK');
              
            } finally {
              client.release();
            }
          }

          async function testDataValidationConstraints() {
            const client = await pool.connect();
            
            try {
              // Create a test user first
              await client.query('BEGIN');
              
              const userResult = await client.query(`
                INSERT INTO users (email, username, password_hash, first_name, last_name)
                VALUES ('test@example.com', 'testuser', 'hashedpassword', 'Test', 'User')
                RETURNING id
              `);
              
              const userId = userResult.rows[0].id;
              
              // Test email validation
              try {
                await client.query(`
                  INSERT INTO contacts (user_id, first_name, last_name, email)
                  VALUES ($1, 'Test', 'Contact', 'invalid-email')
                `, [userId]);
                throw new Error('Email validation should have failed');
              } catch (error) {
                if (error.message.includes('check constraint') || error.message.includes('email_format')) {
                  console.log('✅ Email validation constraint working');
                } else {
                  throw error;
                }
              }
              
              await client.query('ROLLBACK');
              
            } finally {
              client.release();
            }
          }

          async function testConcurrentTransactions() {
            // Simulate concurrent updates to the same record
            const promises = [];
            
            // Create test data first
            const client = await pool.connect();
            await client.query('BEGIN');
            
            const userResult = await client.query(`
              INSERT INTO users (email, username, password_hash, first_name, last_name)
              VALUES ('concurrent@example.com', 'concurrent', 'password', 'Concurrent', 'User')
              RETURNING id
            `);
            const userId = userResult.rows[0].id;
            
            const contactResult = await client.query(`
              INSERT INTO contacts (user_id, first_name, last_name, email)
              VALUES ($1, 'Concurrent', 'Contact', 'concurrent.contact@example.com')
              RETURNING id
            `, [userId]);
            const contactId = contactResult.rows[0].id;
            
            await client.query('COMMIT');
            client.release();
            
            // Now test concurrent updates
            for (let i = 0; i < 10; i++) {
              promises.push(updateContactConcurrently(contactId, i));
            }
            
            const results = await Promise.allSettled(promises);
            const successful = results.filter(r => r.status === 'fulfilled').length;
            const failed = results.filter(r => r.status === 'rejected').length;
            
            console.log(`✅ Concurrent transactions: ${successful} successful, ${failed} failed (expected some failures)`);
          }

          async function updateContactConcurrently(contactId, iteration) {
            const client = await pool.connect();
            
            try {
              await client.query('BEGIN');
              
              // Simulate processing time
              await new Promise(resolve => setTimeout(resolve, Math.random() * 100));
              
              await client.query(`
                UPDATE contacts 
                SET notes = 'Updated by iteration ' || $1 || ' at ' || NOW()
                WHERE id = $2
              `, [iteration, contactId]);
              
              await client.query('COMMIT');
              return iteration;
              
            } catch (error) {
              await client.query('ROLLBACK');
              throw error;
            } finally {
              client.release();
            }
          }

          async function testDataConsistencyUnderLoad() {
            const promises = [];
            const startTime = Date.now();
            
            // Create multiple users and contacts simultaneously
            for (let i = 0; i < 50; i++) {
              promises.push(createUserWithContacts(i));
            }
            
            const results = await Promise.allSettled(promises);
            const successful = results.filter(r => r.status === 'fulfilled').length;
            
            console.log(`✅ Created ${successful}/50 users with contacts under load`);
            
            // Verify data consistency
            const client = await pool.connect();
            const userCount = await client.query('SELECT COUNT(*) FROM users');
            const contactCount = await client.query('SELECT COUNT(*) FROM contacts');
            
            console.log(`Final counts: ${userCount.rows[0].count} users, ${contactCount.rows[0].count} contacts`);
            client.release();
          }

          async function createUserWithContacts(index) {
            const client = await pool.connect();
            
            try {
              await client.query('BEGIN');
              
              const userResult = await client.query(`
                INSERT INTO users (email, username, password_hash, first_name, last_name)
                VALUES ($1, $2, 'password', $3, $4)
                RETURNING id
              `, [
                `load${index}@example.com`,
                `loaduser${index}`,
                `Load${index}`,
                'User'
              ]);
              
              const userId = userResult.rows[0].id;
              
              // Create 3 contacts for each user
              for (let j = 0; j < 3; j++) {
                await client.query(`
                  INSERT INTO contacts (user_id, first_name, last_name, email)
                  VALUES ($1, $2, $3, $4)
                `, [
                  userId,
                  `Contact${j}`,
                  `ForUser${index}`,
                  `contact${index}-${j}@example.com`
                ]);
              }
              
              await client.query('COMMIT');
              return { userId, contactsCreated: 3 };
              
            } catch (error) {
              await client.query('ROLLBACK');
              throw error;
            } finally {
              client.release();
            }
          }

          async function testAuditTrailIntegrity() {
            const client = await pool.connect();
            
            try {
              // Check if audit triggers are working
              await client.query('BEGIN');
              
              const userResult = await client.query(`
                INSERT INTO users (email, username, password_hash, first_name, last_name)
                VALUES ('audit@example.com', 'audituser', 'password', 'Audit', 'User')
                RETURNING id
              `);
              const userId = userResult.rows[0].id;
              
              // Insert should trigger audit log
              const auditResult = await client.query(`
                SELECT COUNT(*) FROM audit.audit_logs 
                WHERE entity_type = 'users' AND entity_id = $1 AND action = 'create'
              `, [userId]);
              
              if (parseInt(auditResult.rows[0].count) > 0) {
                console.log('✅ Audit trail working for INSERT operations');
              } else {
                console.log('⚠️ Audit trail not working for INSERT operations');
              }
              
              // Test UPDATE audit
              await client.query(`
                UPDATE users SET first_name = 'Updated' WHERE id = $1
              `, [userId]);
              
              const updateAuditResult = await client.query(`
                SELECT COUNT(*) FROM audit.audit_logs 
                WHERE entity_type = 'users' AND entity_id = $1 AND action = 'update'
              `, [userId]);
              
              if (parseInt(updateAuditResult.rows[0].count) > 0) {
                console.log('✅ Audit trail working for UPDATE operations');
              } else {
                console.log('⚠️ Audit trail not working for UPDATE operations');
              }
              
              await client.query('COMMIT');
              
            } finally {
              client.release();
            }
          }

          runDataIntegrityTests();
          EOF

          node data-integrity-test.js

      - name: Generate data integrity report
        run: |
          echo "## Data Integrity Test Results" >> $GITHUB_STEP_SUMMARY
          echo "✅ All data integrity tests completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Tests Performed" >> $GITHUB_STEP_SUMMARY
          echo "- Foreign key constraint validation" >> $GITHUB_STEP_SUMMARY
          echo "- Data validation constraints (email format, etc.)" >> $GITHUB_STEP_SUMMARY
          echo "- Concurrent transaction handling" >> $GITHUB_STEP_SUMMARY
          echo "- Data consistency under load" >> $GITHUB_STEP_SUMMARY
          echo "- Audit trail integrity verification" >> $GITHUB_STEP_SUMMARY

  # Backup and restore testing
  backup-restore:
    name: Backup & Restore Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'backup-restore' || github.event_name == 'schedule' }}

    steps:
      - uses: actions/checkout@v4

      - name: Start services
        run: |
          cp .env.example .env
          docker-compose up -d
          sleep 30

      - name: Wait for services and seed data
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 2; done'

          # Create test data for backup
          cat > create-backup-data.js << 'EOF'
          const http = require('http');

          async function createBackupData() {
            const baseUrl = 'http://localhost:3001';
            
            // Create test user
            const registerResponse = await fetch(`${baseUrl}/api/auth/register`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                email: 'backup@test.com',
                username: 'backupuser',
                password: 'BackupTest123!',
                firstName: 'Backup',
                lastName: 'User'
              })
            });
            
            if (registerResponse.ok) {
              const { data } = await registerResponse.json();
              const token = data.token;
              
              // Create test contacts
              for (let i = 0; i < 10; i++) {
                await fetch(`${baseUrl}/api/contacts`, {
                  method: 'POST',
                  headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${token}`
                  },
                  body: JSON.stringify({
                    firstName: `BackupContact${i}`,
                    lastName: 'Test',
                    email: `backup${i}@example.com`,
                    company: 'BackupTestCorp',
                    notes: `Test data for backup validation - contact ${i}`
                  })
                });
              }
              
              console.log('✅ Backup test data created');
            }
          }

          createBackupData().catch(console.error);
          EOF

          node create-backup-data.js

      - name: Create database backup
        run: |
          echo "## Backup & Restore Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Create database dump
          docker exec connectkit-db pg_dump -U admin connectkit > backup-test.sql

          if [ -f "backup-test.sql" ] && [ -s "backup-test.sql" ]; then
            BACKUP_SIZE=$(wc -l < backup-test.sql)
            echo "✅ **Database backup created**: ${BACKUP_SIZE} lines" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Database backup failed**" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Test backup restoration
        run: |
          # Stop services
          docker-compose down -v

          # Start fresh database
          docker-compose up -d db
          sleep 30

          # Restore from backup
          docker exec -i connectkit-db psql -U admin connectkit < backup-test.sql

          # Start other services
          docker-compose up -d
          sleep 30

          # Verify restoration
          timeout 60 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 2; done'

      - name: Verify backup data integrity
        run: |
          # Login and check if data was restored correctly
          cat > verify-restore.js << 'EOF'
          async function verifyRestore() {
            const baseUrl = 'http://localhost:3001';
            
            try {
              // Login with restored user
              const loginResponse = await fetch(`${baseUrl}/api/auth/login`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  email: 'backup@test.com',
                  password: 'BackupTest123!'
                })
              });
              
              if (loginResponse.ok) {
                const { data } = await loginResponse.json();
                const token = data.token;
                
                // Check contacts
                const contactsResponse = await fetch(`${baseUrl}/api/contacts`, {
                  headers: { 'Authorization': `Bearer ${token}` }
                });
                
                if (contactsResponse.ok) {
                  const contactsData = await contactsResponse.json();
                  const contactCount = contactsData.data.contacts.length;
                  
                  console.log(`✅ Restored ${contactCount} contacts successfully`);
                  
                  // Verify specific contact
                  const testContact = contactsData.data.contacts.find(c => 
                    c.firstName === 'BackupContact0' && c.company === 'BackupTestCorp'
                  );
                  
                  if (testContact) {
                    console.log('✅ Specific test contact data verified');
                  } else {
                    console.log('⚠️ Test contact data may be incomplete');
                  }
                } else {
                  console.log('❌ Failed to retrieve contacts after restore');
                }
              } else {
                console.log('❌ Failed to login with restored user data');
              }
            } catch (error) {
              console.error('Error verifying restore:', error.message);
            }
          }

          verifyRestore();
          EOF

          node verify-restore.js

          echo "✅ **Backup restoration verified**" >> $GITHUB_STEP_SUMMARY
          echo "- User data successfully restored" >> $GITHUB_STEP_SUMMARY
          echo "- Contact data integrity confirmed" >> $GITHUB_STEP_SUMMARY
          echo "- Authentication system working post-restore" >> $GITHUB_STEP_SUMMARY

      - name: Test incremental backup scenario
        run: |
          echo "### Incremental Backup Test" >> $GITHUB_STEP_SUMMARY

          # Create additional data after restore
          cat > create-incremental-data.js << 'EOF'
          async function createIncrementalData() {
            const baseUrl = 'http://localhost:3001';
            
            const loginResponse = await fetch(`${baseUrl}/api/auth/login`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                email: 'backup@test.com',
                password: 'BackupTest123!'
              })
            });
            
            if (loginResponse.ok) {
              const { data } = await loginResponse.json();
              const token = data.token;
              
              // Create incremental data
              for (let i = 10; i < 15; i++) {
                await fetch(`${baseUrl}/api/contacts`, {
                  method: 'POST',
                  headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${token}`
                  },
                  body: JSON.stringify({
                    firstName: `IncrementalContact${i}`,
                    lastName: 'Test',
                    email: `incremental${i}@example.com`,
                    company: 'IncrementalCorp',
                    notes: `Incremental data created after restore - contact ${i}`
                  })
                });
              }
              
              console.log('✅ Incremental data created');
            }
          }

          createIncrementalData().catch(console.error);
          EOF

          node create-incremental-data.js

          # Create incremental backup
          docker exec connectkit-db pg_dump -U admin connectkit > backup-incremental.sql

          INCREMENTAL_SIZE=$(wc -l < backup-incremental.sql)
          ORIGINAL_SIZE=$(wc -l < backup-test.sql)

          if [ "$INCREMENTAL_SIZE" -gt "$ORIGINAL_SIZE" ]; then
            echo "✅ **Incremental backup successful**: Backup grew from ${ORIGINAL_SIZE} to ${INCREMENTAL_SIZE} lines" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Incremental backup**: Size didn't increase as expected" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload backup files
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backup-test-files
          path: |
            backup-test.sql
            backup-incremental.sql

      - name: Stop services
        if: always()
        run: docker-compose down -v

  # Nightly report consolidation
  nightly-report:
    name: Nightly Test Report
    runs-on: ubuntu-latest
    needs: [extended-performance, chaos-testing, data-integrity, backup-restore]
    if: always()

    steps:
      - name: Download all nightly test artifacts
        uses: actions/download-artifact@v4
        with:
          path: nightly-results/

      - name: Generate comprehensive nightly report
        run: |
          echo "# 🌙 Nightly Extended Testing Report" > nightly-report.md
          echo "" >> nightly-report.md
          echo "**Test Date:** $(date -u)" >> nightly-report.md
          echo "**Test Duration:** Extended (~2 hours)" >> nightly-report.md
          echo "" >> nightly-report.md

          echo "## 📊 Test Suite Results" >> nightly-report.md
          echo "" >> nightly-report.md

          # Check job results
          if [ "${{ needs.extended-performance.result }}" == "success" ]; then
            echo "✅ **Extended Performance Testing**: All thresholds met" >> nightly-report.md
          elif [ "${{ needs.extended-performance.result }}" == "skipped" ]; then
            echo "⏭️ **Extended Performance Testing**: Skipped" >> nightly-report.md
          else
            echo "❌ **Extended Performance Testing**: Failed or timeout" >> nightly-report.md
          fi

          if [ "${{ needs.chaos-testing.result }}" == "success" ]; then
            echo "✅ **Chaos Engineering**: System resilient to failures" >> nightly-report.md
          elif [ "${{ needs.chaos-testing.result }}" == "skipped" ]; then
            echo "⏭️ **Chaos Engineering**: Skipped" >> nightly-report.md
          else
            echo "❌ **Chaos Engineering**: System instability detected" >> nightly-report.md
          fi

          if [ "${{ needs.data-integrity.result }}" == "success" ]; then
            echo "✅ **Data Integrity**: All constraints and consistency checks passed" >> nightly-report.md
          elif [ "${{ needs.data-integrity.result }}" == "skipped" ]; then
            echo "⏭️ **Data Integrity**: Skipped" >> nightly-report.md
          else
            echo "❌ **Data Integrity**: Data consistency issues found" >> nightly-report.md
          fi

          if [ "${{ needs.backup-restore.result }}" == "success" ]; then
            echo "✅ **Backup & Restore**: Backup and recovery procedures working" >> nightly-report.md
          elif [ "${{ needs.backup-restore.result }}" == "skipped" ]; then
            echo "⏭️ **Backup & Restore**: Skipped" >> nightly-report.md
          else
            echo "❌ **Backup & Restore**: Backup or restore process failed" >> nightly-report.md
          fi

          echo "" >> nightly-report.md
          echo "## 🎯 System Health Assessment" >> nightly-report.md
          echo "" >> nightly-report.md

          # Overall assessment
          SUCCESS_COUNT=$(echo "${{ needs.extended-performance.result == 'success' }} ${{ needs.chaos-testing.result == 'success' }} ${{ needs.data-integrity.result == 'success' }} ${{ needs.backup-restore.result == 'success' }}" | tr ' ' '\n' | grep -c "true")
          TOTAL_TESTS=4

          if [ $SUCCESS_COUNT -eq $TOTAL_TESTS ]; then
            echo "🟢 **Overall System Health: EXCELLENT** ($SUCCESS_COUNT/$TOTAL_TESTS tests passed)" >> nightly-report.md
            echo "The system demonstrated excellent stability, performance, and resilience during extended testing." >> nightly-report.md
          elif [ $SUCCESS_COUNT -ge 3 ]; then
            echo "🟡 **Overall System Health: GOOD** ($SUCCESS_COUNT/$TOTAL_TESTS tests passed)" >> nightly-report.md
            echo "The system is performing well with minor issues in some areas." >> nightly-report.md
          elif [ $SUCCESS_COUNT -ge 2 ]; then
            echo "🟠 **Overall System Health: FAIR** ($SUCCESS_COUNT/$TOTAL_TESTS tests passed)" >> nightly-report.md
            echo "The system has some stability or performance concerns that should be addressed." >> nightly-report.md
          else
            echo "🔴 **Overall System Health: POOR** ($SUCCESS_COUNT/$TOTAL_TESTS tests passed)" >> nightly-report.md
            echo "The system has significant issues that require immediate attention." >> nightly-report.md
          fi

          echo "" >> nightly-report.md
          echo "## 🚀 Performance Insights" >> nightly-report.md
          echo "" >> nightly-report.md
          echo "- Extended load testing with realistic user patterns" >> nightly-report.md
          echo "- Memory leak detection and resource monitoring" >> nightly-report.md
          echo "- Database performance under sustained load" >> nightly-report.md
          echo "- API response time degradation analysis" >> nightly-report.md

          echo "" >> nightly-report.md
          echo "## 🛡️ Resilience Findings" >> nightly-report.md
          echo "" >> nightly-report.md
          echo "- Chaos engineering simulated real-world failures" >> nightly-report.md
          echo "- Database and Redis connection failure recovery" >> nightly-report.md
          echo "- System behavior under resource pressure" >> nightly-report.md
          echo "- Cascading failure prevention mechanisms" >> nightly-report.md

          echo "" >> nightly-report.md
          echo "## 📋 Recommendations" >> nightly-report.md
          echo "" >> nightly-report.md

          if [ $SUCCESS_COUNT -lt $TOTAL_TESTS ]; then
            echo "### Immediate Actions Needed" >> nightly-report.md
            echo "- Investigate failed test results in detail" >> nightly-report.md
            echo "- Review system logs for error patterns" >> nightly-report.md
            echo "- Consider scaling infrastructure if performance issues detected" >> nightly-report.md
            echo "" >> nightly-report.md
          fi

          echo "### Ongoing Maintenance" >> nightly-report.md
          echo "- Monitor trends in nightly test results" >> nightly-report.md
          echo "- Regular backup procedure validation" >> nightly-report.md
          echo "- Performance baseline updates as system evolves" >> nightly-report.md
          echo "- Chaos engineering scenarios expansion" >> nightly-report.md

          cat nightly-report.md >> $GITHUB_STEP_SUMMARY

      - name: Upload consolidated nightly report
        uses: actions/upload-artifact@v4
        with:
          name: nightly-comprehensive-report
          path: |
            nightly-report.md
            nightly-results/

      - name: Send notification on failures
        if: ${{ (needs.extended-performance.result == 'failure') || (needs.chaos-testing.result == 'failure') || (needs.data-integrity.result == 'failure') || (needs.backup-restore.result == 'failure') }}
        run: |
          echo "🚨 NIGHTLY TEST FAILURES DETECTED"
          echo "One or more nightly tests have failed. Please check the detailed results."
          echo "Failed tests should be investigated and resolved promptly."

          # In a real environment, you might send Slack/email notifications here
          # curl -X POST -H 'Content-type: application/json' --data '{"text":"Nightly tests failed"}' $SLACK_WEBHOOK_URL
